#!/usr/bin/env python


from mathdown import *

# print(equation.transform_string(r"""$$
# W(\gamma):=\sum_{k\in \mathcal{Z}}\sum_{\gamma(x_i)=k}\|x_i-\mu_k\|^2,
# $$
# """))

# raise

print(chapter.transform_string(r"""# 聚类

这一章开始介绍一种无监督学习：聚类。聚类就是分类的无监督版本。从某种意义上说，分类最终也是聚类，因为训练样本中标签的设置不总是天然的，也只是人类思维的产物，而在人类的思维活动之中没有明确的类别线索。

聚类算法要实现从无标签数据到一个特定分类器的映射。这个分类器对每个数据点打上类别标签。通俗地说，聚类就是对数据给出一个合理的划分。

作为一种无监督学习模型，聚类模型一般总是依赖数据和类别标签的联合分布$P(X,Z|\theta)$，其中$Z$是类别标签随机变量。主要任务依然是估计未知参数$\theta$。可是无监督学习没有$Z$的观测值，即只有$X$的观测值$x_i$，但没有对应的$y_i$，$i=1,\cdots, N$。我们也可以这样“编故事”：这里原本有一个有标签的数据，并且可以进行常规的监督学习，但是有人不小心弄丢了所有标签，那么我们是否还能找回这些标签。当前的任务是找回所有标签。第一个困难就是在只观测到$X$的情况下估计$\theta$。我们意识到联合分布暗含边缘分布$P(X|\theta)$，而这个分布是一个标准的统计学模型，进而存在$\theta$的估计，即极大边缘似然估计（MMLE）。虽然指明了估计$\theta$的基本路线，但是在实际问题中边缘分布的相关计算将成为另一个困难。本章主要介绍两类经典的聚类方法，它们都避免了直接计算MMLE。

在$\theta$已知的前提下，聚类模型的预测就和分类模型没有分别了。因此，我们不打算讨论聚类的预测任务。

*注* 笔者尽量不用“聚类器”这个词，因为它的意思就是聚类算法，而且聚类模型中已经暗含了一个分类器，作为其判别函数。

**约定** 类别标签默认用$1,\cdots, K$编号。

## K-means聚类

K-means聚类是最早的一类聚类算法。大家熟知的解释并不是基于统计学的，而是构造所谓的“**能量函数**”直接评价每种聚类的优劣。能量函数的最小点代表了最佳聚类。正如一般的机器学习模型总和某个统计模型存在联系，K-means聚类也和统计模型有联系。本节，我们先来认识能量函数，然后给出其包括统计模型在内的多种等价形式。

**约定** $\|x\|$表示由内积$\langle \cdot,\cdot\rangle$导出的范数，在Euclidean空间$\R^n$中，可以代表任何形如$x^TAx$的二次型，其中$A$是一个给定的对称正定矩阵。对应的Frobenius 矩阵范数为$\|X\|_F:=\mathrm{tr}(X^TAX)$。分类器是映射$\gamma:\mathcal{X}\to \mathcal{Z}$，其中$\mathcal{X}$是样本空间，$\mathcal{Z}$是类别标签集。$\mathcal{Z}$元素个数，称为聚类数，总用$K$表示。$N_z$始终表示被标记为$z$类的子样本大小，即$N(\gamma(x_i)=z)$。

### 模型与算法
如前文所述，聚类没有标签数据，不能像分类问题那样构造基于预测的损失函数来判断分类器本身的好坏。尽管如此，我们依然可以为点集$\mathcal{X}\subset\R^p$上的一个分类器$\gamma$构造损失函数，即能量函数，
$$
W(\gamma):=\sum_{k\in \mathcal{Z}}\sum_{\gamma(x_i)=k}\|x_i-\mu_k\|^2,
$$
其中$\mu_k$是$k$类的中心，即点集$\{x_i|\gamma(x_i)=k\}$的均值$\frac{1}{N_k}\sum_{\gamma(x_i)=k}x_i$，称为**聚类中心/质心(centroid)**。能量反映分类器划分出的每一类内部相似程度。K-means聚类遵循ERM原则，此处可称为“能量最小化原则”：合理的分类应使能量最小。

**定义（K-means聚类模型）**
聚类中心$\{\mu_k\}$是分类器$\gamma$唯一的参数(两者相互决定)：
$$
\gamma(x,\{\mu_k\})=\argmin_k\|x-\mu_k\|
$$
能量函数也可写成下述参数形式：
$$
W(\{\mu_k\})= \sum_{k\in \mathcal{Z}}\sum_{\gamma(x_i,\{\mu_k\})=k}\|x_i-\mu_k\|^2
$$
根据所谓的能量最小化原则，K-means 聚类模型归结为下述优化问题，
$$
\min_\gamma W(\gamma)=\min_{\{\mu_k\}} W(\{\mu_k\})
$$

表面上，能量函数是一个简单的二次型，但实际上优化问题(X.X)是一个NP难问题，一般只有近似解。你会发现这里有一个根本的难题：聚类中心是依赖分类结果的；而分类器又要通过聚类中心对数据点分类。解开这个“学习/参数估计-预测-死循环”的方法倒也简单：先用简单的方法猜测聚类中心，再用这些聚类中心对数据点分类(预测)，接着根据分类结果跟新聚类中心（参数估计）。如此循环直到收敛。这种风格的算法被统称为**预测-估计（交替）迭代算法**。下面给出具有这类风格的著名的K-means算法。

**K-means 算法（Lloyd算法）**

输入: 点集$X=\{x_i\}$
返回: 分类器（或聚类中心）

1. 初始化聚类中心(模型参数)$\{\mu_k,k\in \mathcal{Z}\}$；
2. 预测：重置分类器$\gamma(x)=\argmin_{k\in \mathcal{Z}} \|x-\mu_k\|$或分类结果；
3. 估计：更新聚类中心$\mu_k=\frac{1}{N_k}\sum_{\gamma(x_i)=k}x_i$；
4. 重复 2-3 直到收敛；

*注* 预测步中，$\argmin_{k\in \mathcal{Z}} \|x-\mu_k\|$不一定是唯一的。此时可以随机选择一个“最优标签”，或者按照类别标签的特定排序选择其中最小的一个。

*注* K-means算法也是一种近似算法，只能得到局部最优解，而且局部最优解对初始聚类中心较为敏感。最简单的聚类中心初始化方案是，从训练数据中随机选择$|\mathcal{Z}|$个点。高效的k-means++算法改进了初始化策略：尽可能增大初始聚类中心之间的距离。笔者认为，输入空间$\mathcal{X}$不一定仅限于观测到的数据，而是整个Euclidean空间（或者数据的凸包）。因此可先对整个输入空间进行合理分割，在每个分割区域里选择一个合适的点作为聚类中心。

*注* 目前尚未讨论Kmeans聚类的统计模型。算法X.X中，我们使用“点集”这个称谓，而不是“样本”，甚至没有强调独立性。

*注* K-means聚类模型的意义体现在算法中；“K-means聚类”这个词即代表一个模型，也代表一个算法。机器学习专家们似乎非常在意“模型导向”和“算法导向”的分野。

设$R=\{r_{ik}\}:N\times K$为指示矩阵。定义一个二元函数，
$$
Q(\{\mu_k\},R):=\sum_{ik}r_{ik}\|x_i-\mu_k\|^2\\
=\sum_{k}\sum_{r_{ik}=1}\|x_i-\mu_k\|^2
$$

**事实**
二元函数$Q$和能量函数$W$有如下简单关系：
$$W(\{\mu_k\})\leq Q(\{\mu_k\},R)$$ 
且等式成立，当且仅当$R$是分类器$\gamma$的**0-1响应矩阵**，即
$$
r_{ik}:=\begin{cases}1,\gamma(x_i,\{\mu_k\})=k\\
0, \text{否则}\end{cases}
$$

<!-- *注* Q函数的另一种等价形式是，
$$
Q(\{\mu_k\},\{r_{ik}\}):=\sum_{k}\sum_{r_{ik}=1}\|x_i-\mu_k\|^2
$$ -->

你可以认为$Q$也是风险函数，但应注意它含有模型之外的参数。虽然它扩大了参数空间，似乎增加了优化难度，但是这个函数不仅让我们多了一个角度解读K-Means算法，还可以启发我们设计新的算法。

**事实**
优化问题(X.X)等价于最小化(X.X)的二元函数$Q$，而K-means 算法就是$Q$的坐标下降法。（$Q$的变量$R$和$\{\mu_k\}$的优化分别对应于算法X.X的“预测”和“估计”。）

这个事实非常清楚地说明，K-means 算法不能保证收敛到全局最优解。

*注* 这种将一元优化问题(X.X)转化成一个等价的二元优化问题，是一种重要的求解技巧。$Q$也被称为$W$的**极大化函数**(见第X.X节)。这种函数将在第X章扮演重要角色。

### 模型等价形式

#### 无中心等价形式

通过简单的代数运算，可以得到能量函数的等价表达式。

**事实**：在任意的（实）内积空间$H$中，有

$$
\sum_{ij}\|x_i-x_j\|^2=2N\sum_i\|x_i-\mu\|^2,\\
\sum_{i\neq j}\langle x_i, x_j\rangle-(N-1)N\|\mu\|^2=-N\sum_i\|x_i-\mu\|^2,
$$

其中$x_i\in H, i=1,\cdots,N$且满足$\frac{1}{N}\sum_ix_i=\mu$（范数由内积导出）。

这些等式暗示，我们并不需要知道每个点的坐标，而是只需知道两点之间的距离或者内积就可以对所有数据点聚类。等式(X.X)甚至可以消去聚类中心：$\min_\gamma W(\gamma)$等价于最小化下述目标函数，
$$
J(\gamma):=\sum_{k\in \mathcal{Z}}\frac{1}{N_k}\sum_{\gamma(x_i)=\gamma(x_j)=k}\|x_i-x_j\|^2 \\
\propto\sum_{i}\|x_i\|^2-\sum_{k\in \mathcal{Z}} \frac{1}{N_k}\sum_{\gamma(x_i)=\gamma(x_j)=k}x_i\cdot x_j
$$
这个目标函数是不依赖聚类中心的，可称为“无中心形式”。类似地，引入对应的二元函数，
$$
Q(\gamma,\gamma'):=\sum_{k\in \mathcal{Z}}\sum_{\gamma'(x_j)=k}\frac{1}{N_k}\sum_{\gamma(x_i)=k}\|x_i-x_j\|^2 \\
=\sum_{i}\|x_i\|^2+\sum_{k\in \mathcal{Z}}\frac{N_k'}{N_k}\sum_{\gamma(x_i)=k}\|x_i\|^2-\sum_{k\in \mathcal{Z}} \frac{1}{N_k}\sum_{\gamma(x_i)=\gamma'(x_j)=k}x_i\cdot x_j 
$$
其中$N_k$和$N_k'$是分类器$\gamma$和$\gamma'$下$k$类子样本的大小。

**事实**
优化问题(X.X)等价于优化问题(X.X)，也等价于最小化(X.X)的二元函数$Q$。

现在假定每个类的数据量是相近的，或者每个类别有相等的先验概率。这个朴素而合理的假设可称为**均匀先验假设**。

定义点集的**总能量**为
$$
T:=\sum_{i,j}\|x_i-x_j\|^2=W(\gamma)+B(\gamma)
$$
其中**类间能量**$B(\gamma):=\sum_{\gamma(x_i)\neq \gamma(x_j)}\|x_i-x_j\|^2$，而$W$相应地称为**类内能量**。

给定数据$\{x_i\}$，$T$是常数。因此我们有，

**事实** 在均匀先验假设下，$\min_\gamma W(\gamma)$等价于$\max_\gamma B(\gamma)$。（这也是一个无中心形式。）

*注* 更精确地说，上述几个无中心形式只依赖于样本间的距离或者可代表相似度的内积。称这类模型/学习为**度量模型/学习（Metric Learning）**[]。

#### 矩阵分解

K-means聚类可以执行"编码-解码任务"：$\gamma(x)$是分类器对数据点$x$给出的编码；相应地，映射$k\mapsto\mu_k$则是解码；**重构变换**
$$
T: x\mapsto \mu_{\gamma(x)}
$$
作用于所有数据上，将数据点用所在类的中心替换。这个替换过程就如同在一本给定的密码本里查找密码。因此称聚类中心集$\{\mu_k,k\in \mathcal{Z}\}$为**字典(dictionary)/代码簿(codebook)**。

重构变换$T$实现了数据压缩：不需要存储整个数据，而是存储字典和每个数据点的标签。这样的压缩自然会造成信息的损失（有损压缩），其中损失由原数据点和其重构的误差$\|x_i-T(x_i)\|$衡量，即**重构误差**（也可称为**编码损失**或**数据压缩损失**）：
$$
J(T):=\sum_i\|x_i-T(x_i)\|^2=\|X-TX\|_F^2
$$
其中矩阵$TX$由$T(x_i),i=1,\cdots,N$以行向量形式叠成。K-means算法就是在最小化编码损失，或者说寻找最优字典。

现将$x_i$作为行向量拼接成设计矩阵$X:N\times p$，并将所有聚类中心$\mu_k$拼接成聚类中心矩阵$M:K\times p$。能量函数自然被记为$W(M)$。

**事实**
给定0-1响应矩阵$R:N\times K$，且$D$是$K$阶对角矩阵且对角线元素是相应类的样本数。则我们有，
$$
M=D^{-1}R^TX\\
TX=RD^{-1}R^TX=RM\\
W(M)=\|X-TX\|_F^2
$$

*注* 注意(x.x)中$TX$仅是$T(x_i)$以行向量形式叠成的矩阵。重构变换$T$并不是线性变换，即$T\neq RD^{-1}R^T$。

**事实** K-Means聚类等价于用坐标下降法求解下述矩阵分解问题，
$$
\min_{R,M}\|X-RM\|_F,
$$
其中$R:N\times K$是指示矩阵。

#### 自编码模型
根据上一节讨论，我们提出一个有用模型。

**定义（自编码器）**
设$\mathcal{X}$是样本空间，$\mathcal{Z}$是编码空间，则两个空间之间的一对映射
$$
(\phi:\mathcal{X}\to \mathcal{Z},\psi:\mathcal{Z}\to \mathcal{X})
$$
可构成一个“编码-解码模型”，其中$\phi$和$\psi$分别为编码器和解码器。如果$\phi,\psi$是未知的，要通过样本估计，而不是人为设置，那么要求解下述优化问题，
$$
\min_{\phi,\psi} \sum_{i}l(x_i, \psi(\phi(x_i))),
$$
其中$\{x_i\}$是样本，$l$是合理的损失函数。这被称为编码损失最小化原则。此时称该模型为 **（自适应的或自动化的编码-解码模型）**，简称**自编码器**。

*注* $\phi,\psi$应满足一定约束，而不是任意的，尽管定义x.x中没有明确指出这一点。


**事实**
K-means模型是一个自编码器：
$$(\gamma, k\mapsto\mu_k)$$
其中样本空间和编码空间分别是数据空间$\mathcal{X}$和类别标签集$\mathcal{Z}$。

从(X.X)的形式上看，自编码器的参数估计相当于用样本$\{(x_i,x_i)\}$训练机器学习模型$y\sim \psi(\phi(x))$。这种训练过程通常称为**自监督学习**。

**定义 自监督学习(self-supervised learning)**
样本形式为$\{(x_i,x_i)\}$的监督学习称为（基于样本$\{x_i\}$的）自监督学习。进行自监督学习的模型本身是无监督的，因为不依赖数据标签。

#### 等价形式总结
这里总结一下K-means 模型的所有等价形式。
**事实**
$$
\min_\gamma W(\gamma)\\
\iff\max_\gamma B(\gamma) ~(\text{在均匀先验假设下})\\
\iff\min_\gamma J(\gamma)\\
\iff \min_T\sum_i\|x_i-Tx_i\|^2,T:x\mapsto\mu_{\gamma(x)}\\
\iff \min_{R,\{\mu_k\}} Q(\{\mu_k\},R)\\
\iff \min_{\gamma,\gamma'} Q(\gamma,\gamma')\\
\iff \min_{R,M}\|X-RM\|_F
$$
其中$R$是指示矩阵。

#### K-means 的统计解释
K-means聚类似乎不是统计学模型。直到目前，我们都没有提到联合分布。下一节会证明它是所谓的Gaussian 混合模型的极限形式。[]已为K-means聚类给出一个简单的统计解释。设$X$表示取值于样本空间$\mathcal{X}$上的随机变量，$Z$表示取值于类别标签集合$\mathcal{Z}$上的随机变量，服从Categorical 分布。我们的目标是，最小化下述条件方差（原则X.X）：
$$
Var(X|Z,\gamma):=\sum_{k}E(\|X-EX\|^2|Z=k,\gamma)P(Z=k|\gamma)
$$
或者其参数形式
$$
Var(X|Z,\{\mu_k\})=\sum_{k}E(\|X-\mu_k\|^2|Z=k,\{\mu_k\})P(Z=k|\{\mu_k\})
$$
显然(X.X)样本形式就是能量函数$W(\gamma)$，其中分类器$\gamma$建立样本间的映射关系$x_i\mapsto k_i$。

#### 向量量级化

K-Means编码的一个著名应用是：**向量量级化（VQ）**。我们知道量级化是用来把连续数据用近似的离散数据表示，也就是把每个连续数据用与之接近的整数表示。K-Means编码无非是把$\R^n$上的数据点用有限点集上某个与之最近的元素表示。这是“向量量级化”这个名称的由来。

VQ常用于图像处理。一个图像被看作一个完整数据，其中每个数据点是图像像素点（或块）的编码。以RGB格式的图像为例，数据点的维数为3，而2X2的像素块维度为12。根据K-Means编码原理，每个像素点（块）都有一个类别标签，用来从字典中索引对应的像素点（块）。重构时，图像中的每个像素点（块）都被字典中$K$个像素点（块）替代。虽然重构图像会缺失很多次要细节，但是存储图像所需的信息却很少。这就实现了图像压缩。若$b\times b$的像素块作为一个数据点，则一张RGB格式的图像的VQ压缩率约为$\frac{\log_2 K}{24b^2}$。


### 聚类数$K$的选择
聚类数$K$的选择也得斟酌一下。太小的$K$，使分类过于粗糙，区分不了明显分离的子集；太大的$K$，会对点集产生过细的划分。我们知道$W(\gamma)$的最小值只取决于$K$，而且是$K$的减函数，记为$W^*(K)$。一般$W^*(K)$的图像总是呈现出弯曲的手臂形状。该手臂图的肘部就是我们选用的值。此外，还可以利用所谓的Gap 统计量更客观地确定聚类数。

一种自动控制聚类数的技术是，[]提出的**X-Means 算法**。它的具有启发性的idea是：一个类如果有较大的类内能量，那么可能包含过多的点，因此可以进一步被分成两部分，而聚类中心和聚类数也随之增加。

### K-medoids 算法

K-means算法的简单变体是K-medoids 算法。它将Kmeans算法的第3步，计算聚类中心，改为计算**medoids**。

在K-medoids 算法中，我们将用到一般的“距离函数”，代替2-范数导出的距离。这里的距离函数不是严格意义上的距离。它包含任何被广泛接受的能够衡量数据点之间相似度或差异度的函数，如：
1. $\R^n$上，$\|x-y\|_p,0\leq p\leq \infty$
2. 内积空间上，$1-\frac{x\cdot y}{\|x\|_2\|y\|_2}$
3. 加权图/网络上，节点$x$到$y$的最短路径长度
4. 标准单纯形$\Delta^{K-1}$上，交叉熵$H(x,y)$或KL散度$D_{KL}(x\|y)$

**定义 （medoid）**
点集$S$的medoid，$\mu_S$被定义为，
$$
\mu_S:=\argmin_{\mu\in S}\sum_{x\in S} d(x,\mu),
$$
其中$d$是距离函数。$d(x,\mu_S)$衡量了$S$的**分散度**。

*注* 笔者认为 medoids 更合理的定义是这样的：给定距离空间$\mathcal{X}$，其子集$S$的medoid为
$$
\argmin_{x\in \mathcal{X}}\sum_{x'\in S} d(x,x')
$$
即可在$\mathcal{X}$中搜索medoids。K-medoids 算法中规定只在$S$内搜索$S$的medoid。

K-medoids分类器$\gamma$的能量函数定义为所有类的分散度之和，即
$$
W(\gamma) := \sum_{k\in \mathcal{Z}} \sum_{\gamma(x_i)=k} d(x_i,\mu_k),
$$
其中$\mu_k$是点集$\{\gamma(x_i)=k\}$的medoid，$\{x_i\}$是样本。


**定义（K-medoids聚类模型）**
定义参数分类器，
$$
\gamma(x,\{\mu_k\})=\argmin_k d(x,\mu_k)
$$
其中$\{\mu_k\}$是模型的未知参数。和K-means算法类似，K-medoids 算法本质上是在最小化能量函数，即
$$
\min_\gamma W(\gamma)= \min_{\{\mu_k\}}\sum_{k\in \mathcal{Z}} \sum_{\gamma(x_i,\{\mu_k\})=k} d(x_i,\mu_k)
$$


**K-medoids 算法**
输入：点集$\{x_i\}$（距离空间中的数据点）
返回：分类器（或medoids）
1. 初始化聚类medoids, $\{\mu_k,k\in \mathcal{Z}\}$；
2. 重置分类器$\gamma(x)=\argmin_{k\in \mathcal{Z}} d(x,\mu_k)$或分类结果；
3. 更新medoids, $\mu_k=\argmin_{\gamma(x)=k}\sum_{\gamma(x_i)=k} d(x,x_i), k\in \mathcal{Z}$；
4. 重复 2-3 直到收敛；


定义二元函数，
$$
Q(\{\mu_k\},R):=\sum_{ik}r_{ik}d(x_i,\mu_k)
$$
其中$R=\{r_{ik}\}$是指示矩阵。我们有如下事实。
**事实**
优化问题(X.X)等价于最小化$Q$ (X.X)，而K-medoids 算法就是$Q$的坐标下降法。

K-medoids 算法比 K-means 算法适用范围更广泛。首先，它对数据空间和距离函数$d$没有明确的限制，不需要依赖于特定的代数结构。其次，该算法完全依赖于点-点之间的距离，而不直接依赖于数据点的内在特征，可以处理非数值型数据。

K-medoids 聚类没有明确的统计解释，但你可以构造统计泛函
$$
D(X|Z,\{\mu_k\}):=\sum_{k}E(d(X,\mu_k)|Z=k)P(Z=k)
$$
其样本形式就是损失函数(X.X)。


K-medoids 算法的一个有趣应用是**网络分析**：数据是一个网络（加权图）；数据点是这个网络上的节点，一般不能用 Euclidean 坐标直接表示；节点间有距离，常被定义为路径长度，但和 Euclidean 距离很不同；而类的中心也只能从节点中选取。此时，K-means 算法就基本没有用武之地了，除非事先把数据点嵌入到Euclidean空间中。

### 中心形式与无中心形式
无论 K-means 算法，还是 K-medoids 算法，都是构造下述优化问题来进行聚类：
$$
\min_\gamma W(\gamma),\gamma:X\to \mathcal{Z},
$$
其中$W$是评估聚类的某个准则，习惯地称为能量函数。根据上文的讨论，能量函数大概有两种设计方案：一种是中心形式，如能量函数（X.X）和(X.X)；另一种是无中心形式，如能量函数（X.X）。对于K-medoids 算法，我们也可以构造“无中心能量函数”：
$$
J(\gamma) := \sum_{k\in \mathcal{Z}} \frac{1}{N_k}\sum_{x_i,x_j\in \gamma^{-1}(k)} d(x_i,x_j)
$$
相应地，构造二元函数，
$$
Q(\gamma,\gamma') := \sum_{k\in \mathcal{Z}} \sum_{\gamma'(x_i)=k}\frac{1}{N_k}\sum_{\gamma(x_i)=k} d(x_i,x_j)
$$

然而，和K-mean算法不同，(X.X)的最优分类器不一定是(X.X)的最优分类器，即两个能量函数是不绝对等价的。不过两者的解不会有太大差别。

无中心形式使我们彻底摆脱了对聚类中心（centroid或medoid）概念的依赖，构造出非参数模型。相应的算法也不需要计算聚类中心，因此是一个只有“预测”的迭代过程。

**算法 无中心聚类算法**
输入：点集$\{x_i\}$（距离空间中的数据点）
返回：分类结果$D$
1. 初始化数据$D=\{(x_i,z_i)\}$；
2. 根据数据$D$和“无中心分类器”（另见X.X节）
   $$
   x\mapsto\argmin_{k\in \mathcal{Z}} \frac{1}{N_k}\sum_{z_i=k} d(x,x_i)
   $$
   重置分类结果$\{(x_i,z_i')\}$，并更新数据$D\leftarrow\{(x_i,z_i')\}$；
3. 重复 2 直到收敛；

算法X.X第2步中，映射(X.X)显然也可以看成数据$D$上的变换，即更新$D$。存在两种更新策略：串行和并行。串行是在给每个数据点$x_i$分配标签的同时，更新数据$D$，其中计算顺序会影响计算结果；而并行是在基于旧的有标签数据$D$给所有数据点$x_i$分配标签后，更新数据。笔者真正推荐的是小批量法：每次选择一个子数据，并行地给数据点分配标签，然后更新数据。

**事实**
算法X.X等价于用坐标下降法最小化(X.X)。

## 聚类的统计模型

现在讨论聚类的统计模型，其定义完全参考分类模型。

**定义 (聚类的一般统计模型)**
作为非监督学习模型，聚类模型需要设定联合分布：
$$
P(X,Z=k)=P(X|k) p(k),k=1,\cdots, K.
$$
其中类别标签$Z$是不可观测的，即隐变量。习惯上可用$X$的边缘分布$\sum_kP(X|k)p(k)$形式地表示一个聚类模型。

聚类是分类的无监督形式，有和Bayes分类器一样的联合分布，因此也可以叫做“Bayes聚类(器)”。

类似于分类模型，我们一般只考虑参数解绑形式：
$$
\sum_k P(X|k,\theta_1) P(k|\theta_2).
$$

混合模型（见定义X.X）是一个重要例子，
$$
\sum_k p_k(x) \pi_k,
$$
其中$\theta=\{\pi_k,p_k(\cdot), k=1,\cdots,K\}$。显然，混合模型的无监督学习等价于混合分布的MLE。

当观测到独立样本$\{x_i\}$时，有对数似然，
$$
l(\theta)=\sum_i\ln \big(\sum_kp_k(x_i)\pi_k\big).
$$

想象一下，$p_k(x)$都是单峰函数，即该分布只有一个最大值，且对应的最大点出发的任何一条射线上概率值递减，如多元Gaussian分布。最大化(X.X)会迫使同类样本分布在最大点附近；第$k$类分布的“峰”（概率值较大的区域）大致覆盖第$k$类数据点。这些峰自然形成对数据点的划分。

然而，直接最大化式（10）并不方便。下一章将详细介绍一种高效的通用算法，用于解决混合模型问题。

### Gaussian 混合模型

Gaussian 混合模型（GMM）是一种特殊混合模型，常用于聚类问题。GMM 由 Gaussian 混合分布刻画。虽然X.X节已经介绍了这种分布，但是我们要在这里给出明确的定义。

**定义（Gaussian 混合分布/GMM）**
Gaussian 混合分布/GMM是以Gaussian分布$N(\mu_k,\Sigma_k), k=1,\cdots,K$为基分布的混合分布/混合模型：
$$
P(X|\theta)=\sum_k N(X; \mu_k,\Sigma_k)\pi_k,
$$
其中$\sum_k\pi_k=1,\pi_k\geq 0$，$\theta$包含参数$\mu_k,\Sigma_k,\pi_k,k=1,\cdots,K$。

*注* $X$服从Gaussian混合分布可以表示为，$X\sim \sum_k\pi_k N(\mu_k,\Sigma_k)$，尽管这不是很规范。

*注* GMM 就是 LDA/QDA 无监督版本。类似于LDA/QDA，它也可以根据协方差矩阵分成几种类型。

根据无监督学习的设定，我们的任务是计算Gaussian分布混合的MLE。然而，GMM没有直接最大化似然函数 (X.X)，而是最小化下述二元函数：

$$
Q(\{\pi_k,\mu_k,\Sigma_k\},R):=-\sum_{ik}r_{ik}(\ln p(z|\mu_k,\Sigma_k)+\ln \pi_k)\\
\sim \sum_{ik}r_{ik}(\frac{1}{2}\|x_i-\mu_k\|_{\Sigma_k^{-1}}^2+\frac{1}{2}\ln \det\Sigma_k-\ln \pi_k),
$$

其中$\{x_i,i=1,\cdots,N\}$是样本，$R=\{r_{ik}\}$为随机矩阵/概率响应矩阵。最小化(X.X)的关键在于寻找$\mu_k$的最优解，即
$$
\min_{\{\mu_k\},\{r_{ik}\}} \sum_{ik}r_{ik}\|x_i-\mu_k\|_{\Sigma_k^{-1}}^2
$$

(X.X)的目标函数显然是K-means能量函数的推广。(X.X)的最小化或优化问题(X.X)也没有闭合形式的解，只能通过迭代算法得到满意解。更要注意，$Q$的全局最小点也不一定是参数的MLE。

我们写出GMM聚类算法。该算法的严格推导将在第X章给出。

**GMM聚类算法**

输入：独立样本$X=\{x_i\}$
返回：分类器（或参数）
1. 初始化分类器参数$\{\theta_k\}$；
2. 预测：重置分类器$\gamma(x)=\argmin_k p(x|\mu_k,\Sigma_k)\pi_k$，关键是计算响应概率$p(k|x)\sim p(x|\mu_k,\Sigma_k)\pi_k$；
3. 估计：更新参数，设权重$r_{ik}=p(k|x_i)\sim p(k|x_i,\mu_k,\Sigma_k)\pi_k$，
$$
   \mu_k\leftarrow\sum_ir_{ik}x_i /\sum_ir_{ik};\\
\Sigma_k\leftarrow\sum_ir_{ik}(x_i-\mu_k)(x_i-\mu_k)^T/ \sum_ir_{ik};\\
\pi_k \leftarrow \sum_ir_{ik}/\sum_{ik}r_{ik};
$$
1. 重复 2-3 直到收敛；

这个算法非常并不难理解，依然采用”预测-估计交替风格”，其中预测结果是一个概率，即“随机预测”。
<!-- 这个算法非常容易理解。首先，响应概率$p(k|x)\sim p(k|x,\mu_k,\Sigma_k)\pi_k$对每个样本分配权重。定义响应矩阵$R=\{r_{ik}=p(k|x_i)\}$，其列归一化记为$R_1$。然后更新参数：聚类中心$\mu_k$是样本加权求和，可写成矩阵形式
$$
M={R}_1^TX
$$
其中矩阵$M:K\times p$的每一行为聚类中心；$\Sigma_k$和$\pi_k$的计算原理同$\mu_k$。 -->

**事实**
K-means聚类是GMM的极限形式。(另见文献[]图14.7)

*证明* 现考虑GMM中$\Sigma_k=\sigma^2$，即所有协方差矩阵都是相同的数量矩阵，其中$\sigma^2$已知且充分小。设$\|x-\mu_k\|<\|x-\mu_l\|$，则响应概率比

$$
\frac{P(Z=l|x,\theta)}{P(Z=k|x,\theta)}=\frac{\pi_l}{\pi_k}e^{\frac{\|x-\mu_k\|^2-\|x-\mu_l\|^2}{\sigma^2}}\to 0.
$$

因此GMM分类器对输入$x$的预测结果为，$\gamma(x)=\argmin_k \|x-\mu_k\|$，其中参数$\pi_k>0$不起作用。这意味着，在这个设定下，GMM退化为K-means聚类；GMM聚类算法也退化为Kmeans算法。


### 概率型自编码器
作为K-means算法的推广形式，GMM也能实现自编码器，但和Kmeans算法实现的决定型自编码器不同，GMM实现的是“概率型自编码器”。这类模型的编码器和解码器本质上都是条件概率，即响应概率$p(z|x)$和生成概率$p(x|z)$（可分别称为“编码分布”和“解码分布”）。判断一个自编码器是否是概率型的关键看编码分布是否是非退化的。具体定义见X.X节。

## 聚类决策模型
综合K-means聚类和聚类统计模型，我们提出一个通用的基于统计决策的聚类模型。该模型其实是一种框架，给出具体的设置后，可以导出多种新型模型，其功能不限于聚类。

### 定义与算法框架

**定义 聚类决策模型**
聚类决策模型有如下损失函数确定，
$$
Q(\theta,\{w_{ik}\}):=\sum_{ik}w_{ik}l_k(x_i, \theta)
$$
其中其中$l_k(x_i, \theta)$是样本点$x_i$相对第$k$类的损失，权重$w_{ik}\geq 0$（默认满足$\sum_{k}w_{ik}=1$）。如果权重$w_{ik}=0$或$1$，那么称之为**硬聚类决策模型**，否则为**软聚类决策模型**。硬聚类决策的损失函数可写为，
$$
Q(\theta,\gamma):=\sum_{k}\sum_{\gamma(x_i)=k}l_k(x_i, \theta)
$$

*注* 从(X.X)的形式上看，聚类决策模型就是把数据分成几类，并对每一类独立（有公共参数或没有）构造一个统计决策模型。因此，也可以称之为“**分组统计决策模型**”。

(X.X)的一种合理的参数解绑形式为，
$$
Q(\{\theta_k\},\{w_{ik}\}):=\sum_{ik}w_{ik}l(x_i, \theta_k)
$$
对应的硬模型为，
$$
Q(\{\theta_k\},\{w_{ik}\}):=\sum_{k}\sum_{\gamma(x_i)=k}l(x_i, \theta_k)
$$

利用坐标下降法，给出(X.X)和(X.X)的算法框架。
**算法 (软聚类决策算法)**
输入：点集$\{x_i\}$
返回：参数$\theta$
1. 初始化参数$\{\theta_k\}$；
2. 重置权重矩阵, $w_{ik}\sim e^{-l(x_i,\theta_k)}$；
3. 更新参数, $\theta_k\leftarrow\argmin_{\{\theta_k\}}\sum_{ik}w_{ik}l(x_i, \theta_k)$；
4. 重复 2-3 直到收敛；

**算法 (硬聚类决策算法)**
输入：点集$\{x_i\}$
返回：参数$\theta$
1. 初始化参数$\{\theta_k\}$；
2. 重置分类器, $\gamma(x)=\argmin_{k} l(x,\theta_k)$或分类结果；
3. 更新参数, $\theta_k\leftarrow\argmin_{\theta_k}\sum_{\gamma(x_i)=k} l(x_i,\theta_k)$；
4. 重复 2-3 直到收敛；

### 模糊C-means聚类
聚类显然是一种聚类决策。K-means聚类是**硬聚类（决策）**，而GMM是**软聚类**。本节介绍软聚类令一个非常著名的经典例子。

**定义 模糊C-means聚类（FCM）**。
FCM由下述损失函数确定：
$$
Q(\{\mu_k\},\{w_{ik}\}):=\sum_{ik}w_{ik}^m\|x_i−\mu_k\|^2,
$$
其中$w_{ik}$被解释为$x_i$对第$k$类的隶属度（所有隶属度构成**隶属度矩阵**），$m$是控制模糊程度的超参数。

隶属度的计算有**隶属度函数**给出$w_{ik}=w_k(x_i)$，而隶属度函数的设计比较随意，一般采用，
$$
w_{k}(x):=\frac{1}{\sum_{l}(\frac{\|x-\mu_k\|}{\|x-\mu_l\|})^{\frac{2}{m-1}}}
$$

**算法 FCM算法**
输入：点集$\{x_i\}$，设置超参数$m$
返回：分类器（或聚类中心）
1. 初始化聚类中心$\{\mu_k\}$；
2. 更新隶属度:
$$
w_{ik}\leftarrow\frac{1}{\sum_{l}(\frac{\|x_i-\mu_k\|}{\|x_i-\mu_l\|})^{\frac{2}{m-1}}}\sim \|x_i-\mu_k\|^{-\frac{2}{m-1}}.
$$
1. 更新中心，$\mu_k\leftarrow\sum_iw_{ik}^mx_i /\sum_iw_{ik}^m$；
2. 重复 2-3 直到收敛；

算法运行结束后，给出聚类结果：$\gamma(x_i)=\max_kw_{ik}$。在迭代过程中，合理调整超参数$m$，平衡收敛速度和解的最优性。

**事实**
FCM迭代过程近似为(X.X)的坐标下降法。

**事实**
K-means聚类也是FCM的极限形式。(具体地说，当$m \to 1$时，$w_{ik} \to 0$或$1$，即零隶属度矩阵退化为0-1响应矩阵，同时FCM退化为K-means聚类。)

形式上，FCM算法和GMM聚类算法是很相似的。但在FCM算法中，权重没有唯一的计算规则，即隶属度函数没有唯一的设计。即使(X.X)不是最好的隶属度函数，这也不妨碍它的有效性。

## 其他话题

### 中心-分类器
这一小节本该放在X.X章。但由于历史原因，笔者把它放在这里。

K-means算法，以及框架X.X，第3步都暗含了一个监督算法或者分类器。我们可以为这个分类器给出一个非常一般的定义。(其实已经包含在K-medoids聚类中了。)

**定义 中心-分类器**
在数据空间$\mathcal{X}$和标签集$\mathcal{Y}$上，构造参数分类器
$$
\gamma(x,\{\mu_k\})=\argmin_k d(x,\mu_k):\mathcal{X}\to\mathcal{Y}
$$
其判别函数可以是$\delta_k(x)= - d(x,\mu_k)$，其中未知参数$\mu_k,k=1,\cdots,K$被称为分类器的中心。（$d$是$\mathcal{X}$上的距离函数，并可包含未知参数。）

对中心-分类器来说，对一个数据点进行分类，就是在判断它与中心的相似度，换言之，一个类的中心是这个类的“全信息代表性元素”。根据模型的中心和距离函数的具体定义，可以构造不同的分类器，如K-means聚类对应的分类器的中心是用每个类数据点的代数平均，而它的距离是 Euclidean 距离。LDA也是中心-分类器 (带有先验项)，其中心同K-means聚类，而距离是Mahalanobis距离；K-means中心-分类器是LDA的极限形式。


类似地，给出对应的无中心形式。

**定义 无中心-分类器**
$$
\gamma(x)=\argmin_k \frac{1}{N_k}\sum_{x_i:k} d(x,x_i):\mathcal{X}\to\mathcal{Y}
$$

无中心-分类器没有需要估计的参数（除非聚类$d$中包含未知参数），也就是没有常规的学习算法，而是简单保存训练数据，并直接对新数据进行预测。它看上去像是一个懒汉，因此被称为**惰性模型**（或**延迟模型**）。以后我们还会遇到这类算法。

*注* 显然无中心-分类器建立了有标签数据上的映射：$(X,Y)\mapsto (X,\hat{Y})$。对给定数据不断作用这个映射即可实现无中心-聚类算法。


### 原型分类法

考查$K$-分类问题。对每个类中的样本进行聚类(第$k$类样本被分为$R_k$类)，其中聚类中心被称为**原型(prototype)**。第$k$类有$R_k$个聚类中心/原型；共有$\sum_kR_k$个原型。分类器学习这些原型，而不是所有样本。可期望分类器的效率得到提升，而精度不会受到较大的影响，甚至可以克服过拟合。这就是**原型分类法**。对应的分类器称为**原型分类器**。

最简单的原型分类器是判断任意输入和原型的距离：
$$
\gamma(x)=\argmin_k \{\min_{x'\in R_k}d(x,x')\},
$$
即若与输入$x$最近的原型属于$k$类，则$x$预测为$k$类，其中$d$是一个合理的距离函数。显然当$R_k=1$时，这个原型分类器退化为中心-分类器。因此中心-分类器也称**单原型分类器**。

显然，我们也可以构造“原型聚类器”。

### 异常点检测
聚类模型非常适合用来做异常点检测。我们知道任何聚类模型都由一个经验风险$J(\theta;X)$决定，其中$X$是样本。如果从$X$中删除某个点可以使风险大幅度降低，那么就可以把该点视为异常点。基于这个直觉我们先给出一个检测异常点的通用模型。

**定义 异常点检测**
给定风险函数$J(\theta;X)$。$s$-异常点检测是指下述优化问题：
$$
 \min_{S,|S|\leq s,\theta}J(\theta;X\setminus S)
$$
其中$s$事先给定或者通过某种算法自适应地确定。样本点集$S\subset X$称为异常点集。

将风险函数换成能量函数(x.x)，得到基于KMeans聚类的异常点检测。

**定义 KMeans异常点检测**
设$W(\{\mu_k\};X)$是KMeans的能量函数。$s$-Kmeans异常点检测是指下述优化问题：
$$
 \min_{S,|S|\leq s,\{\mu_k\}}W(\{\mu_k\};X\setminus S)
$$

(x.x)优化问题的最优值和$s$有关，记为$L(s)$。这显然是一个递减函数。类似于聚类数的确定，我们通过观察函数$L(s)$的图像，确定合适的$s$。

<!-- 直接遍历所有$|S|\leq s$的子样本$S$是耗时的。 -->
(x.x)中的目标函数是二元函数，并且如果考虑用能量函数等价形式(x.x)，那么它将是三元函数。首先被考虑到的优化策略是坐标下降法。算法的具体设计留给读者。

### 半监督聚类

无监督学习的一个小问题是，没有绝对的类别标签。比如聚类算法可以根据性别将人类分成两类，但是并不能决定用“男性”或者“女性”给某个类打标签。如果其中一类中个别样本有类别标签，那么就可以帮助人们决定整个类的标签。那些有标签的数据称为有标签数据；其余为无标签数据。一般来说，无标签数据要远多于有便签数据，甚至有标签数只是为了使最优分类可唯一确定。半监督学习就是用来处理这样的数据的。

<center>半监督学习的数据格式，其中无标签数据$x^0_i$要远多于有标签数据$(x^1_i,y_i^1)$</center>

| $X$   | $Y$|
| ----- | ---- |
|$x^1_i$ | $y_i^1$    |
|$x^0_i$ | ?    |

构造Kmeans聚类的半监督版本很简单。这相当于解下述优化问题：

$$
\min_\gamma W(\gamma)= \sum_{z\in C}\sum_{x_i,x_j:z}\|x_i-x_j\|^2\\
\gamma(x_i)=z_i, x_i\in D^1,
$$

其中$D^1$和$D^0$分别是有类别便签和无类别便签的样本，$x:z$表示$D^0\cup D^1$中任何被$\gamma$判为$z$类的样本。

仿照K-means算法，容易写出相应的半监督 K-means 的迭代算法。下面给出一种可行的方案。

**半监督 K-means 算法**

输入：有标签数据$D^1=\{(x_i,z_i)\}$, 无标签数据$D^0=\{x_i'\}$
返回：分类器（或聚类中心）

令$\mathcal{Z}^1$是$D^1$中出现过的标签集, $\mathcal{Z}^0=\mathcal{Z}\setminus \mathcal{Z}^1$是$D^1$中未出现的标签集；

1. 根据$D^1$，初始化聚类中心$\{\mu_z,z \in \mathcal{Z}^1\}$（直接取$D^1$中每一类的均值），再根据$D^0$，初始化聚类中心$\{\mu_z,z\in \mathcal{Z}^0\}$；
2. 重置分类器/分类结果，$\gamma(x)=\argmin_{z\in C} \|x-\mu_z\|,x\in D^0$；
3. 更新聚类中心$\mu_z=\frac{1}{N_z}\sum_{\gamma(x_i)=z}x_i,x_i\in D^0\cup D^1$；
4. 重复 2-3 直到收敛；

*注* 半监督 K-means 算法和 K-means 算法的主要不同是，第2步不改变$D^1$的标签/分类结果。

*注* 出于方便，用$x\in D^1$表示$x$是有标签的，即存在对应有标签样本点$(x,z)\in D^1$。

这个半监督学习算法唯一复杂的是初始化。如果$\mathcal{Z}^1$不包括所有类别，那么首先给$\mathcal{Z}^0$中的类指定对应的聚类中心，如在$D^0$中随机选择，然后$D^1$中每个类的中心作为$\mathcal{Z}^1$中的类别标签对应的聚类中心。如果$\mathcal{Z}^1$包括所有类别，那么直接确定$D^1$的聚类中心即可（这是一个监督分类问题）。

因为半监督学习模型提供了有标签的数据，所以监督学习有时确实可以完成任务，即从有标签数据中学习参数，然后预测无标签数据，但那么做太平凡了，而且当有标签数据很少时，监督学习的效果会很差。

---

*练习*

1. 请根据K-means聚类的一些改进版本，提出（无）中心-分类器的改进版本，并给出一些应用。
2. 证明K-means聚类的最小能量$W^*(K), K=1,\cdots, N$关于聚类数$K$严格递减，其中$N$是数据点个数。
3. 实现用VQ压缩图像，其中数据点是图像$b\times b$的像素块，其中$b$是算法参数。如果原图像大小不是$b\times b$的正数倍，那么请问如何处理？
5. 构造K-medoids 算法的半监督学习版本。获取一份社会网络数据，对该网络中的节点进行聚类。
6. 令点$x$到点集$S$的距离定义为$d(x,S)=\min_{x'\in S}d(x,x')$，证明K-medoids 算法等价于
   $$
   \min_{\mu_k}\sum_id(x_i,\{\mu_k\})
   $$
   其中$\mu_k$是中心。
7. GMM中，如果参数$\{\pi_k\}$已知，那么如何简化GMM聚类算法。
8. 构造半监督的软聚类算法，如半监督FCM，半监督GMM。
9.  GMM中，若$\Sigma_k=\Sigma$(tied型)已知，则GMM聚类算法(X.X)等价于矩阵分解
   $$
   \min_{R,M}\|X-RM\|_{\Sigma^{-1}}
   $$
   其中$M:K\times p$是聚类中心矩阵，$R=\{r_{ik}\}:N\times K$是随机矩阵（响应矩阵）。
10. 利用无中心-分类器，试构造不依赖中心的GMM聚类算法。
11. 根据聚类或分类的无中心形式，定义无参数$\theta$的聚类决策模型。
12. 说某个聚类模型$p(x,z|\theta)$是无效的，如果对任何一个观测到的样本$\{x_i\}$和任意两组标签$\{z_i\},\{z_i'\}$（对重复样本满足，若$x_i=x_j$，则$z_i=z_j,z_i'=z_j'$），满足
   $$
   \prod_i p(x_i,z_i|\hat\theta) =  \prod_i p(x_i,z_i'|\hat\theta')
   $$
   其中$\hat\theta$和$\hat\theta'$分别是参数在两个样本$\{(x_i,z_i)\}$和$\{(x_i,z_i')\}$下的MLE（或者是某种由模型指定的估计）。概括地说，一个聚类模型是无效的，如果给样本添加任何标签都得到相同的极大联合似然值。请证明下述Categorical 混合模型是无效的。
   $$
   P(X=l|Z=k)=p_{kl}, P(Z=k) = \pi_k,\\
   l=1,\cdots,L,k=1,\cdots,K.
   $$
   其中$p_{kl},\pi_k$均是合理的概率值。
10 请思考，Logistic回归是不是一种中心-分类器？所有线性分类器是否最终都是中心-分类器？
1.  请读者构造一个具体的基于原型分类法的算法，并用计算机语言实现。


"""
))
